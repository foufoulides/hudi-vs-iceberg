{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "first-aberdeen",
   "metadata": {},
   "source": [
    "# Updating a database with deltas using iceberg and athena\n",
    "\n",
    "In this tutorial we are going to demonstrate how to make a database based on deltas recieved from an external source. We will build a database containing a table of all the raw deltas and then create a second database that shows us the state of the raw table delta at a particular date.\n",
    "\n",
    "We are going to pretend that we recieve a `csv` file that contains changes of a table. We are going to concatenate those deltas into a single table. Then generate a subsequent table based on the \"raw\" deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import datetime\n",
    "import pydbtools as pydb\n",
    "from scripts.create_dummy_deltas import get_dummy_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-speaker",
   "metadata": {},
   "source": [
    "## Setup first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup your own testing area (set foldername = GH username)\n",
    "foldername = \"enter your foldername\" # GH username\n",
    "foldername = foldername.lower().replace(\"-\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"eu-west-1\"\n",
    "bucketname = \"alpha-everyone\"\n",
    "db_name = f\"aws_example_{foldername}\"\n",
    "db_base_path = f\"s3://{bucketname}/{foldername}/database\"\n",
    "s3_base_path = f\"s3://{bucketname}/{foldername}/\"\n",
    "\n",
    "# Delete all the s3 files in a given path\n",
    "if wr.s3.list_objects(s3_base_path):\n",
    "    print(\"deleting objs\")\n",
    "    wr.s3.delete_objects(s3_base_path)\n",
    "\n",
    "# Delete the database if it exists\n",
    "df_dbs = wr.catalog.databases(limit=1000)\n",
    "if db_name in df_dbs[\"Database\"].to_list():\n",
    "    print(f\"deleting database {db_name}\")\n",
    "    wr.catalog.delete_database(\n",
    "        name=db_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-ideal",
   "metadata": {},
   "source": [
    "### Get the deltas\n",
    "\n",
    "We are going to create deltas from the `\"data/employees.csv` table. I am using code in a script in this repo `scripts/create_dummy_deltas.py`. It isn't important what it is doing for this tutorial but if you wanna see what it does you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = get_dummy_deltas(\"data/employees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-acquisition",
   "metadata": {},
   "source": [
    "**Day 1:** the first extract of deltas from our databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas[\"day1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-raleigh",
   "metadata": {},
   "source": [
    "**Day 2:** The next days deltas show that Lexie has their `department_id` and `manager_id` corrected. As well 2 new employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas[\"day2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-verse",
   "metadata": {},
   "source": [
    "**Day 3:** The next days deltas show that:\n",
    "- Dexter has left the department\n",
    "- Robert and Iris have moved departments and are working for Lexie\n",
    "- 3 New employees are also now working for Lexie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas[\"day3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-mediterranean",
   "metadata": {},
   "source": [
    "###Â Create a database and tables\n",
    "\n",
    "There are many ways you can create a database and tables (see other tutorials). For this example we will use awswrangler (which infers the table schema from the data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init database and delta table\n",
    "wr.catalog.create_database(name=db_name)\n",
    "\n",
    "# Add some parameters that will be useful to manage our deltas\n",
    "df = deltas[\"day1\"]\n",
    "df[\"date_received\"] = datetime.date(2021,1,1)\n",
    "\n",
    "# We are going to name the folder the same as our table\n",
    "# this makes things less complex and is adviced\n",
    "table_name = \"raw_deltas\"\n",
    "raw_delta_path = os.path.join(\n",
    "    db_base_path,\n",
    "    table_name\n",
    ")\n",
    "_ = wr.s3.to_parquet(\n",
    "    df,\n",
    "    path=raw_delta_path,\n",
    "    dataset=True,\n",
    "    database=db_name,\n",
    "    table=table_name,\n",
    "    mode=\"append\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"SELECT * FROM {db_name}.{table_name}\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-trader",
   "metadata": {},
   "source": [
    "### Take stock\n",
    "\n",
    "We now have a database that we created once and we initialised our `raw_deltas` table in our database.\n",
    "\n",
    "Now we are going to create an iceberg table using Athena. This table will show what our raw_deltas will look like at each day we do an update.\n",
    "\n",
    "> We are also going to wrap these code chunks into functions. This will help us utilise these functions later to show how you can run a delta update and then the downstream tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6a920-565d-4352-8031-5e480eefc2b6",
   "metadata": {},
   "source": [
    "### Athena iceberg derived table\n",
    "\n",
    "To start off we need to create an empty iceberg table which is registered with the AWS Glue catalog. We'll do this by sending a `CREATE TABLE` query to Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dbee60-7e7c-421e-a150-863b9744759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_iceberg_table(table_name: str):\n",
    "    table_path = os.path.join(\n",
    "        db_base_path,\n",
    "        table_name,\n",
    "    )\n",
    "\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE {db_name}.employee_athena_iceberg(\n",
    "    employee_id int,\n",
    "    sex string,\n",
    "    forename string,\n",
    "    surname string,\n",
    "    department_id int,\n",
    "    manager_id int,\n",
    "    record_created date,\n",
    "    record_last_updated date)\n",
    "    LOCATION '{table_path}/'\n",
    "    TBLPROPERTIES (\n",
    "        'table_type'='ICEBERG',\n",
    "        'format'='parquet'\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        _ = pydb.start_query_execution_and_wait(create_table_sql)\n",
    "    except Exception as e:\n",
    "        if not \"Iceberg table to be created already exists\" in str(e):\n",
    "            raise\n",
    "        else:\n",
    "            print(\"Iceberg table to be created already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6830f52-11b2-4b12-a56a-44ca7c2ff462",
   "metadata": {},
   "source": [
    "Now let's create the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a0d53-aa5e-433a-84a3-fd4f6a80f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_table_name=\"employee_athena_iceberg\"\n",
    "create_empty_iceberg_table(\"employee_athena_iceberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4d172-405b-4f80-8dac-d2ff3c8a0f2e",
   "metadata": {},
   "source": [
    "Let's query our empty table to see that it's been created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45280b41-2fa5-43e9-a6db-4323abded181",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydb.read_sql_query(f\"SELECT * FROM {db_name}.{iceberg_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e805d-3508-4003-882b-e8209110c563",
   "metadata": {},
   "source": [
    "We're now going to create a function which will use our `raw_deltas` table to:\n",
    "* insert new records\n",
    "* delete records which are marked as deleted in the `record_deleted` column\n",
    "* update the manager_id and department_id fields if either of these have changed\n",
    "\n",
    "We'll do all of this with the `MERGE INTO` SQL command for iceberg tables in Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f03de-5f83-4a68-8eb2-f7af639db580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_athena_iceberg(report_date: str, table_name: str):\n",
    "    full_sql = f\"\"\"\n",
    "    MERGE INTO {db_name}.{table_name} t USING (\n",
    "        SELECT \n",
    "            employee_id,\n",
    "            sex,\n",
    "            forename,\n",
    "            surname,\n",
    "            department_id,\n",
    "            manager_id,\n",
    "            date '{report_date}' AS record_last_updated,\n",
    "            record_deleted\n",
    "        FROM\n",
    "        (\n",
    "            SELECT *,\n",
    "            row_number() OVER (PARTITION BY employee_id ORDER BY date_received DESC) as rn\n",
    "            FROM {db_name}.raw_deltas\n",
    "            WHERE date_received <= date '{report_date}'\n",
    "        )\n",
    "        WHERE rn = 1\n",
    "    ) s ON (t.employee_id = s.employee_id)\n",
    "        WHEN MATCHED AND s.record_deleted\n",
    "            THEN DELETE\n",
    "        WHEN MATCHED AND NOT s.record_deleted AND (t.department_id != s.department_id OR t.manager_id != s.manager_id)\n",
    "            THEN UPDATE\n",
    "                SET department_id = s.department_id, manager_id = s.manager_id, record_last_updated = s.record_last_updated\n",
    "        WHEN NOT MATCHED\n",
    "            THEN INSERT\n",
    "                (employee_id, sex, forename, surname, department_id, manager_id, record_created, record_last_updated)\n",
    "                    VALUES (s.employee_id, s.sex, s.forename, s.surname, s.department_id, s.manager_id, s.record_last_updated, s.record_last_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    # run the query\n",
    "    pydb.start_query_execution_and_wait(full_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code to create report for 2021-01-01 data\n",
    "create_report_athena_iceberg(\"2021-01-01\", iceberg_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"SELECT * FROM {db_name}.{iceberg_table_name}\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-satellite",
   "metadata": {},
   "source": [
    "### Final bit\n",
    "\n",
    "Now we have 2 tables.\n",
    "\n",
    "- `raw_deltas` a table of all the raw data concatenated\n",
    "- `employee_athena_iceberg` a report based on what employees table looked like at the given point in time. (Remember in this example the raw_deltas are from an external table employees where we get given daily deltas of changes).\n",
    "\n",
    "Now we want to update each of these tables based on the data from day2 then do it again for day3s data. Lets do that now (starting with day 2)\n",
    "\n",
    "### Day2\n",
    "\n",
    "Add day2 data to the deltas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deltas[\"day2\"]\n",
    "df[\"date_received\"] = datetime.date(2021,1,2)\n",
    "\n",
    "_ = wr.s3.to_parquet(\n",
    "    df,\n",
    "    path=raw_delta_path,\n",
    "    dataset=True,\n",
    "    database=db_name,\n",
    "    table=table_name,\n",
    "    mode=\"append\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-transaction",
   "metadata": {},
   "source": [
    "The run the reports for the same date (now the deltas table has been updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report_athena_iceberg(\"2021-01-02\", iceberg_table_name) # note we use insert to now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM {db_name}.{iceberg_table_name}\n",
    "\"\"\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-immune",
   "metadata": {},
   "source": [
    "As we can see new employyes have been added and Lexie's department and manager records have been updated as expected.\n",
    "\n",
    "It is also worth noting that previous reports have been untouched (using the pandas table as an example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-cable",
   "metadata": {},
   "source": [
    "### Day 3\n",
    "\n",
    "Lets run the same again for day 3. The code is exactly the same as it was for day2 but now with a new date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update raw deltas first\n",
    "df = deltas[\"day3\"]\n",
    "df[\"date_received\"] = datetime.date(2021,1,3)\n",
    "\n",
    "_ = wr.s3.to_parquet(\n",
    "    df,\n",
    "    path=raw_delta_path,\n",
    "    dataset=True,\n",
    "    database=db_name,\n",
    "    table=table_name,\n",
    "    mode=\"append\"\n",
    ")\n",
    "\n",
    "# Then run reports\n",
    "create_report_athena_iceberg(\"2021-01-03\", iceberg_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM {db_name}.{iceberg_table_name}\n",
    "\"\"\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-northeast",
   "metadata": {},
   "source": [
    "From the above we can see that Dexter has been removed from the report (as he left) and new staff have been added. Again as expected when looking at our original deltas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772eab5-7b6a-45ed-9652-a5d82a8a0c03",
   "metadata": {},
   "source": [
    "# Performing Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41055874-3c17-4f6f-992b-0c3ef1920141",
   "metadata": {},
   "source": [
    "As we're using iceberg we can perform time travel on our table to view the state of the table at a given point of time. To get a list of when the table was updated so we now when to travel to, we can query the iceberg table's history as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a774809-e7fb-4a3f-80e9-2955c1e383a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pydb.read_sql_query(f'SELECT * FROM \"{db_name}\".\"{iceberg_table_name}$history\"', ctas_approach=False)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e336c-b478-40cc-b188-ef108088e3a5",
   "metadata": {},
   "source": [
    "Let's grab a time between our latest and penultimate change and query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf48d8-9f9b-44aa-a759-c069ee9019af",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = (history.made_current_at[1] + pd.to_timedelta(1, unit='s')).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "pydb.read_sql_query(f\"SELECT * FROM {db_name}.{iceberg_table_name} FOR TIMESTAMP AS OF TIMESTAMP '{timestamp} UTC'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a5d4e-62bd-4a7f-8dc1-415605e8c702",
   "metadata": {},
   "source": [
    "We can also optimise how our iceberg table is stored by running an `OPTIMIZE` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf818c-e737-4fc0-bcb9-6bcb2787adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydb.start_query_execution_and_wait(f\"OPTIMIZE {db_name}.{iceberg_table_name} REWRITE DATA USING BIN_PACK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bf1ba-f6d5-43cb-9b3a-29374233efe4",
   "metadata": {},
   "source": [
    "Note that this does create a new version of our table (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32f3ae-42f5-4320-81b8-5578563f55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pydb.read_sql_query(f'SELECT * FROM \"{db_name}\".\"{iceberg_table_name}$history\"', ctas_approach=False)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-relations",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "\n",
    "So hopefully that is useful. Let's destroy what we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean up\n",
    "\n",
    "# Delete all the s3 files in a given path\n",
    "if wr.s3.list_objects(s3_base_path):\n",
    "    print(\"deleting objs\")\n",
    "    wr.s3.delete_objects(s3_base_path)\n",
    "\n",
    "# Delete the database if it exists\n",
    "df_dbs = wr.catalog.databases(limit=1000)\n",
    "if db_name in df_dbs[\"Database\"].to_list():\n",
    "    print(\"Deleting database\")\n",
    "    wr.catalog.delete_database(\n",
    "        name=db_name\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-tools-demo",
   "language": "python",
   "name": "aws-tools-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
