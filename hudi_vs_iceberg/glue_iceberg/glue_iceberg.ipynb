{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "## Glue + Iceberg evaluation",
			"metadata": {
				"editable": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "###### Bulk Insert \n###### SCD2\n###### Impute deletions\n###### Deduplication\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# %session_id_prefix native-iceberg-dataframe-\n# %glue_version 3.0\n# %idle_timeout 60\n# %%configure \n# {\n#   \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n#   \"--datalake-formats\": \"iceberg\"\n# }",
			"metadata": {
				"trusted": true
			},
			"execution_count": 21,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# catalog_name = \"glue_catalog\"\n# bucket_name = \"sb-test-bucket-ireland\"\n# bucket_prefix = \"sb\"\n# database_name = \"9_iceberg_dataframe\"\n# table_name = \"datagensb\"\n# warehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 22,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Initialise SparkSession",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# from pyspark.sql import SparkSession\n# spark = SparkSession.builder \\\n#     .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n#     .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n#     .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n#     .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n#     .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n#     .getOrCreate()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 23,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Clean up existing resources ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# query = f\"\"\"\n# CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\n# \"\"\"\n# spark.sql(query)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 24,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Create Iceberg tables, insert synthetic data (25K rows)",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# df_products = spark.read.option('header','true').csv(\"s3://sb-test-bucket-ireland/datagen1m/part-00003-69b14150-9456-488a-a533-129cd7024988-c000.csv\")\n# # Convert the date_string column to timestamp\n# #df_products = df_products.withColumn(\"date_timestamp\", to_timestamp(col(\"extraction_timestamp\"), 'yyyy-MM-dd'))\n\n# df_products.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 25,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_products.show(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 26,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_products.sortWithinPartitions(\"PK\") \\\n#     .writeTo(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .create()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 27,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.catalog.listTables(database_name)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 28,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .show(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 29,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}.history\") \\\n#     .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 30,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# ut = time.time()\n\n# product_updates = [\n#     {'PK': 'PKey_0000001', 'SK': 'SKey_qXoVvd876z', 'ParentItemId': 400, 'ItemType': 'Electronics','ItemPrice':'test',Status:,PartNumber:, SerialNumber:,ItemDetails:,'updated_at': ut}, # Update\n#     {'product_id': '00006', 'product_name': 'Chair', 'price': 50, 'category': 'Furniture', 'updated_at': ut} # Insert\n# ]\n# df_product_updates = spark.createDataFrame(Row(**x) for x in product_updates)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 31,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_product_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 32,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# query = f\"\"\"\n# MERGE INTO {catalog_name}.{database_name}.{table_name} AS t\n# USING (SELECT * FROM tmp_{table_name}_updates) AS u\n# ON t.product_id = u.product_id\n# WHEN MATCHED THEN UPDATE SET t.updated_at = u.updated_at\n# WHEN NOT MATCHED THEN INSERT *\n# \"\"\"\n# spark.sql(query)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 33,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "\n# %%sql\n# SELECT * FROM glue_catalog.iceberg_sql.product",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Append ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# df_products_appends = spark.read.option('header','true').csv(\"s3://sb-test-bucket-ireland/datagen1m/part-00008-69b14150-9456-488a-a533-129cd7024988-c000.csv\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# df_products_appends.show(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 34,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#df_products_appends.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 35,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_products_appends.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 36,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .show(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 37,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}.history\") \\\n#     .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 38,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "500416\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# IcebergMergeOutputDF = spark.sql(\"\"\"\n#     MERGE INTO (f\"{catalog_name}.{database_name}.{table_name}\") t\n#     USING (SELECT PK, SK, ParentItemId, product_name, quantity_available, to_timestamp(last_update_time) as last_update_time FROM incremental_input_data) s\n#     ON t.product_id = s.product_id\n#     WHEN MATCHED AND s.op = 'D' THEN DELETE\n#     WHEN MATCHED THEN UPDATE SET t.quantity_available = s.quantity_available, t.last_update_time = s.last_update_time \n#     WHEN NOT MATCHED THEN INSERT (product_id, category, product_name, quantity_available, last_update_time) VALUES (s.product_id, s.category, s.product_name, s.quantity_available, s.last_update_time)\n# \"\"\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Overwrite",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# df_products_ow = spark.read.option('header','true').csv(\"s3://sb-test-bucket-ireland/datagen1m/part-00013-69b14150-9456-488a-a533-129cd7024988-c000.csv\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# df_products_ow.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 40,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_products_ow.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").overwritePartitions()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .show(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 39,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 41,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}.history\") \\\n#     .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 42,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Delete a record",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# df_delete = df_products_ow.where(\"PK==PKey_0000446\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# IcebergMergeOutputDF = spark.sql(\"\"\"\n#     MERGE INTO job_catalog.iceberg_demo.iceberg_output t\n#     USING (SELECT op, product_id, category, product_name, quantity_available, to_timestamp(last_update_time) as last_update_time FROM incremental_input_data) s\n#     ON t.product_id = s.product_id\n#     WHEN MATCHED AND s.op = 'D' THEN DELETE\n#     WHEN MATCHED THEN UPDATE SET t.quantity_available = s.quantity_available, t.last_update_time = s.last_update_time \n#     WHEN NOT MATCHED THEN INSERT (product_id, category, product_name, quantity_available, last_update_time) VALUES (s.product_id, s.category, s.product_name, s.quantity_available, s.last_update_time)\n# \"\"\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Identify deleted ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# from pyspark.sql import Row\n# import time\n# from pyspark.sql.functions import col, to_timestamp,lit\n# ut = time.time()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# df_products = spark.read.parquet(\"s3://mojap-raw-hist-sandbox-608/hmpps/oasys/EOR/BACKUP_SYNTHETIC/OASYS_QUESTION_SANDBOXY/\")\n# # Convert the date_string column to timestamp\n# df_products = df_products.withColumn(\"date_timestamp\", to_timestamp(col(\"extraction_timestamp\"), 'yyyy-MM-dd'))\n# df_products = df_products.withColumn(\"op\", lit(None).cast(\"string\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# df_initial_full_load = df_products.sample(0.001)\n# df_to_insert = df_products.sample(0.001)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_products.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_initial_full_load.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_products=df_products.limit(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_products.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_to_insert.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_initial_full_load.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# table_location = f\"s3://mojap-raw-hist-sandbox-608/hmpps/oasys/EOR/BACKUP_SYNTHETIC/ICEEXAMPLE\"\n# table_location",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_products.sortWithinPartitions(\"scn\") \\\n#     .writeTo(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .create()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.catalog.listTables(database_name)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}.history\") \\\n#     .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# from pyspark.sql.functions import lit\n# ut = time.time()\n\n\n\n\n# # sample 1/5 of the already existing data from the full load inital data\n# df_to_update = df_initial_full_load.sample(0.2)\n# # Update the op column\n# df_to_update = df_to_update.withColumn(\"op\", lit(\"U\"))\n# df_to_update.show(1,vertical=True)\n\n# df_to_update.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# # Combine the updated data with new data (inserts)\n# df_product_updates = df_to_insert.union(df_to_update)\n# df_product_updates.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# ut = time.time()\n# product_updates = [\n#     {'op': 'update', 'extraction_timestamp': '2021-07-18', 'scn': 'SYNTHETICS', 'oasys_question_pk': 0, 'date_timestamp': ut}, # Update\n#     {'op': 'new', 'extraction_timestamp': '2023-07-18', 'scn': 'SYNTHETICS', 'oasys_question_pk': 1, 'date_timestamp': ut} # Insert\n# ]\n# df_product_updates = spark.createDataFrame(Row(**x) for x in product_updates)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_product_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_product_updates.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_product_updates = df_product_updates.withColumn(\"date_timestamp\", to_timestamp(col(\"extraction_timestamp\"), 'yyyy-MM-dd'))\n# df_product_updates = df_product_updates.withColumn(\"op\", lit(None).cast(\"string\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# query = f\"\"\"\n# MERGE INTO {catalog_name}.{database_name}.{table_name} AS t\n# USING (SELECT * FROM tmp_{table_name}_updates) AS u\n# ON t.scn = u.scn\n# WHEN MATCHED THEN UPDATE SET t.date_timestamp = u.date_timestamp\n# WHEN NOT MATCHED THEN INSERT *\n# \"\"\"\n# spark.sql(query)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# df_products.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 20,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Bulk Insert \n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "%session_id_prefix native-iceberg-dataframe-\n%glue_version 3.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\"\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.3 \nSetting session ID prefix to native-iceberg-dataframe-\nSetting Glue version to: 3.0\nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "catalog_name = \"glue_catalog\"\nbucket_name = \"sb-test-bucket-ireland\"\nbucket_prefix = \"sb\"\ndatabase_name = \"n2_iceberg_dataframe\"\ntable_name = \"datagensb\"\nwarehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nCREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\n\"\"\"\nspark.sql(query)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "full_load = spark.read.option('header','true').parquet(\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/full_load.parquet\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as f",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\nfull_load = full_load.withColumn(\"start_datetime\",f.col(\"extraction_timestamp\"))\nfull_load = full_load.withColumn(\"end_datetime\", f.to_timestamp(f.lit(future_end_datetime), 'yyyy-MM-dd'))\nfull_load = full_load.withColumn(\"op\",f.lit(\"None\"))\nfull_load = full_load.withColumn(\"is_current\",f.lit(True))\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# delete later\nfull_load = full_load.withColumn(\"start_datetime\",f.col(\"extraction_timestamp\"))\nfull_load = full_load.withColumn(\"end_datetime\", f.to_timestamp(f.lit(\"2050-01-01\"), 'yyyy-MM-dd'))\nfull_load = full_load.withColumn(\"op\",f.lit(\"None\"))\nfull_load = full_load.withColumn(\"is_current\",f.lit(True))",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "input_filepath = \"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/full_load.parquet\"\noutput_directory = f\"{catalog_name}.{database_name}.{table_name}\"\nfuture_end_datetime = \"2050-01-01\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load=spark.read.option('header','true').parquet(input_filepath)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType, Row\nimport time\n\n\ndef bulk_insert(input_filepath,output_directory,future_end_datetime):\n    start = time.time()\n    full_load=spark.read.option('header','true').parquet(input_filepath)\n    full_load = full_load.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n    full_load = full_load.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n    full_load = full_load.withColumn(\"op\",F.lit(\"None\"))\n    full_load = full_load.withColumn(\"is_current\",F.lit(True))\n    full_load.sortWithinPartitions(\"product_name\") \\\n    .writeTo(output_directory) \\\n    .create()\n    print(time.time()-start)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "bulk_insert(input_filepath,output_directory,future_end_datetime)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "10.97949767112732\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#full_load = full_load.withColumn(\"'end_datetime'\",f.col(\"extraction_timestamp\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#from pyspark.sql.types import StringType,BooleanType,DateType",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#full_load=full_load.withColumn(\"op\",full_load.op.cast(StringType))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "full_load.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+\n|product_id|product_name|price|extraction_timestamp|  op|\n+----------+------------+-----+--------------------+----+\n|     00001|      Heater|  250| 2022-01-01 01:01:01|null|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|null|\n|     00003|  Television|  600| 2022-01-01 01:01:01|null|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|null|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|null|\n+----------+------------+-----+--------------------+----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load.sortWithinPartitions(\"product_name\") \\\n    .writeTo(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .create()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.catalog.listTables(database_name)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "[Table(name='datagensb', database='n1_iceberg_dataframe', description=None, tableType=None, isTemporary=False)]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}.history\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+-----------------+---------+-------------------+\n|     made_current_at|      snapshot_id|parent_id|is_current_ancestor|\n+--------------------+-----------------+---------+-------------------+\n|2023-05-25 10:40:...|78222291571215463|     null|               true|\n+--------------------+-----------------+---------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Slowly Changing Dimension Type 2 (SCD2)\nThe updates are created by replacing one column with the same value to simplify the testing. The soft deletes are not taken into account since very similar process from a performance perspective.\n\nSteps:\n\nRead updates\nJoin full load with updates on primary key\nSet end_datetime to the extraction_timestamp of the updated records\nClose the existing records\nAdd curation columms to updates\nAppend updated data to existing data",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "full_load_updates = spark.read.option('header','true').parquet(\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/updates.parquet\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "full_load_updates = full_load_updates.withColumn(\"start_datetime\",f.col(\"extraction_timestamp\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates = full_load_updates.withColumn(\"end_datetime\", f.to_timestamp(f.lit(\"2050-01-01\"), 'yyyy-MM-dd'))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates = full_load_updates.withColumn(\"is_current\",f.lit(True))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp| op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#from pyspark.sql.types import IntegerType",
			"metadata": {
				"trusted": true
			},
			"execution_count": 42,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#full_load_updates = full_load_updates.withColumn(\"op\",full_load_updates[\"op\"].cast(IntegerType()))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 43,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "full_load_updates.schema",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "StructType(List(StructField(product_id,StringType,true),StructField(product_name,StringType,true),StructField(price,LongType,true),StructField(extraction_timestamp,TimestampType,true),StructField(op,StringType,true),StructField(start_datetime,TimestampType,true),StructField(end_datetime,TimestampType,true),StructField(is_current,BooleanType,false)))\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#full_load_updates.schema",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "full_load.schema",
			"metadata": {
				"trusted": true
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "StructType(List(StructField(product_id,StringType,true),StructField(product_name,StringType,true),StructField(price,LongType,true),StructField(extraction_timestamp,TimestampType,true),StructField(op,StringType,false),StructField(start_datetime,TimestampType,true),StructField(end_datetime,TimestampType,true),StructField(is_current,BooleanType,false)))\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "full_load_updates.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp| op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|  U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\nfull_load_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"\nMERGE INTO {catalog_name}.{database_name}.{table_name} AS f\nUSING (SELECT * FROM tmp_{table_name}_updates) AS u\nON f.product_id = u.product_id\nWHEN MATCHED THEN UPDATE SET f.end_datetime = u.extraction_timestamp, f.is_current = False \n\n\"\"\"\nspark.sql(query)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# query = f\"\"\"\n# SELECT tmp_{table_name}_updates.product_id as mergekey,tmp_{table_name}_updates.*\n# from tmp_{table_name}_updates\n\n# UNION ALL\n\n# SELECT * FROM {catalog_name}.{database_name}.{table_name}\n# ON {catalog_name}.{database_name}.{table_name}.product_id = tmp_{table_name}_updates.product_id\n# WHEN MATCHED THEN UPDATE SET f.end_datetime = u.extraction_timestamp, f.is_current = False\n\n# \"\"\"\n# spark.sql(query)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# query = f\"\"\"\n# SELECT * FROM tmp_{table_name}_updates\n\n\n# UNION ALL\n\n# SELECT * FROM {catalog_name}.{database_name}.{table_name}\n\n\n# \"\"\"\n# spark.sql(query)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 113,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[product_id: string, product_name: string, price: bigint, extraction_timestamp: timestamp, op: string, start_datetime: timestamp, end_datetime: timestamp, is_current: boolean]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 114,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp| op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n|     00004|     Blender|  100| 2022-01-01 01:01:01|  U|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|  U|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|  U|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|  U|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|  U|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n+----------+------------+-----+--------------------+---+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n#     .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater| 1000| 2023-01-01 00:00:00|null|2023-01-01 00:00:00|2050-01-01 00:00:00|     false|\n|     00002|  Thermostat| 1000| 2023-01-01 00:00:00|null|2023-01-01 00:00:00|2050-01-01 00:00:00|     false|\n|     00003|  Television| 1000| 2023-01-01 00:00:00|null|2023-01-01 00:00:00|2050-01-01 00:00:00|     false|\n|     00004|     Blender| 1000| 2023-01-01 00:00:00|null|2023-01-01 00:00:00|2050-01-01 00:00:00|     false|\n|     00005| USB charger| 1000| 2023-01-01 00:00:00|null|2023-01-01 00:00:00|2050-01-01 00:00:00|     false|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|null|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|null|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|null|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|null|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|null|2022-01-01 01:01:01|2023-01-01 00:00:00|     false|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "full_load_updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").createOrReplace()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as f",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import time\nstart = time.time()\nprint(time.time()-start)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "3.4332275390625e-05\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "round(start,1)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "1684516751.7\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "start = time.time()\nfull_load = spark.read.option('header','true').parquet(\"s3://sb-test-bucket-ireland/dummy_data/full_load.parquet\")\nfull_load = full_load.withColumn(\"start_datetime\",f.col(\"extraction_timestamp\"))\nfull_load = full_load.withColumn(\"end_datetime\", f.to_timestamp(f.lit(\"2050-01-01\"), 'yyyy-MM-dd'))\nfull_load = full_load.withColumn(\"op\",f.lit(\"None\"))\nfull_load = full_load.withColumn(\"is_current\",f.lit(True))\nfull_load.sortWithinPartitions(\"product_name\") \\\n    .writeTo(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .create()\nspark.catalog.listTables(database_name)\nfull_load_updates = spark.read.option('header','true').parquet(\"s3://sb-test-bucket-ireland/dummy_data/updates.parquet\")\nfull_load_updates = full_load_updates.withColumn(\"start_datetime\",f.col(\"extraction_timestamp\"))\nfull_load_updates = full_load_updates.withColumn(\"end_datetime\", f.to_timestamp(f.lit(\"2050-01-01\"), 'yyyy-MM-dd'))\nfull_load_updates = full_load_updates.withColumn(\"is_current\",f.lit(True))\n\nfull_load_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\")\nquery = f\"\"\"\nMERGE INTO {catalog_name}.{database_name}.{table_name} AS f\nUSING (SELECT * FROM tmp_{table_name}_updates) AS u\nON f.product_id = u.product_id\nWHEN MATCHED THEN UPDATE SET f.end_datetime = u.extraction_timestamp, f.is_current = False \n\n\"\"\"\nspark.sql(query)\nfull_load_updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()\nprint(time.time()-start)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "45.95531153678894\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.table(f\"{catalog_name}.{database_name}.{table_name}\") \\\n    .show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|product_id|product_name|price|extraction_timestamp|  op|     start_datetime|       end_datetime|is_current|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n|     00004|     Blender|  100| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater|  250| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00003|  Television|  600| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00002|  Thermostat|  400| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00005| USB charger|   50| 2022-01-01 01:01:01|None|2022-01-01 01:01:01|2023-01-01 01:01:01|     false|\n|     00001|      Heater| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00002|  Thermostat| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00003|  Television| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00004|     Blender| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n|     00005| USB charger| 1000| 2023-01-01 01:01:01|   U|2023-01-01 01:01:01|2050-01-01 00:00:00|      true|\n+----------+------------+-----+--------------------+----+-------------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Slowly Changing Dimension Type 2 - Complex\nThis is a more complex SCD2 process which takes into account:\nLate arriving records where an update is processed with an extraction_timestamp that is later than the extraction_timestamp of the last processed record\nBatches which contain multiple updates to the same primary key\nThe process can be summarised as follows:\n\nConcat/union updates with the existing data\nSort by primary key and extraction_timestamp\nWindow by primary key and set the end_datetime to the next record's extraction_timestamp, otherwise set it to a future distant timestamp\nThe process could be optimised by separating records which have not received any updates, but this is left out to make the logic easier to follow.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "late_updates = spark.read.option('header','true').parquet(\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/late_updates.parquet\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "Functions",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType, Row\nimport time\n\n\ndef bulk_insert(input_filepath,output_directory,future_end_datetime):\n    start = time.time()\n    full_load=spark.read.option('header','true').parquet(input_filepath)\n    full_load = full_load.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n    full_load = full_load.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n    full_load = full_load.withColumn(\"op\",F.lit(\"None\"))\n    full_load = full_load.withColumn(\"is_current\",F.lit(True))\n    full_load.sortWithinPartitions(\"product_name\") \\\n    .writeTo(output_directory) \\\n    .create()\n    print(time.time()-start)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "updates_filepath =\"s3://sb-test-bucket-ireland/data-engineering-use-cases/dummy-data/updates.parquet\"\nprimary_key = \"product_id\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def scd2_simple(input_filepath, updates_filepath, output_directory, future_end_datetime, primary_key):\n    start = time.time()\n    full_load_updates = spark.read.option('header','true').parquet(updates_filepath)\n    full_load_updates = full_load_updates.withColumn(\"start_datetime\",F.col(\"extraction_timestamp\"))\n    full_load_updates = full_load_updates.withColumn(\"end_datetime\", F.to_timestamp(F.lit(future_end_datetime), 'yyyy-MM-dd'))\n    full_load_updates = full_load_updates.withColumn(\"is_current\",F.lit(True))\n\n    full_load_updates.createOrReplaceTempView(f\"tmp_{table_name}_updates\")\n    query = f\"\"\"\n    MERGE INTO {catalog_name}.{database_name}.{table_name} AS f\n    USING (SELECT * FROM tmp_{table_name}_updates) AS u\n    ON f.product_id = u.product_id\n    WHEN MATCHED THEN UPDATE SET f.end_datetime = u.extraction_timestamp, f.is_current = False \n\n    \"\"\"\n    spark.sql(query)\n    full_load_updates.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").append()\n    print(time.time()-start)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "scd2_simple(input_filepath, updates_filepath, output_directory, future_end_datetime, primary_key)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "23.577928066253662\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def scd2_complex(input_filepath, updates_filepath, output_directory, future_end_datetime, primary_key):\n    start = time.time()\n    print(time.time()-start)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}