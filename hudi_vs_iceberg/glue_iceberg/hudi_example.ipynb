{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%iam_role arn:aws:iam::684969100054:role/AdminAccessGlueNotebook\n",
    "%region eu-west-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4542fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 3.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 5\n",
    "%extra_jars s3://sb-test-bucket-ireland/hudi-jars/hudi-spark3.1.1-bundle_2.12-0.10.1.jar, s3://sb-test-bucket-ireland/hudi-jars/hudi-utilities-bundle_2.12-0.10.1.jar, s3://sb-test-bucket-ireland/hudi-jars/spark-avro_2.12-3.1.1.jar\n",
    "%%configure \n",
    "{\n",
    "  \"--conf\": \"spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false --conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a520389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33036a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "show databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74586677",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"sb-test-bucket-ireland\"\n",
    "bucket_prefix = \"soumaya-folder\"\n",
    "database_name = \"hudi_sm\"\n",
    "table_name = \"product_cow\"\n",
    "table_prefix = f\"{bucket_prefix}/{database_name}/{table_name}\"\n",
    "table_location = f\"s3://{bucket_name}/{table_prefix}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b3e28",
   "metadata": {},
   "source": [
    "# Clean up existing resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "## Create a database with the name hudi_df to host hudi tables if not exists.\n",
    "try:\n",
    "    glue = boto3.client('glue')\n",
    "    glue.create_database(DatabaseInput={'Name': database_name})\n",
    "    print(f\"New database {database_name} created\")\n",
    "except glue.exceptions.AlreadyExistsException:\n",
    "    print(f\"Database {database_name} already exist\")\n",
    "\n",
    "## Delete files in S3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.objects.filter(Prefix=f\"{table_prefix}/\").delete()\n",
    "\n",
    "## Drop table in Glue Data Catalog\n",
    "try:\n",
    "    glue = boto3.client('glue')\n",
    "    glue.delete_table(DatabaseName=database_name, Name=table_name)\n",
    "except glue.exceptions.EntityNotFoundException:\n",
    "    print(f\"Table {database_name}.{table_name} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8aa84",
   "metadata": {},
   "source": [
    "## Create Hudi table with sample data using catalog sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213362c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import time\n",
    "\n",
    "ut = time.time()\n",
    "\n",
    "product = [\n",
    "    {'product_id': '00001', 'product_name': 'Heater', 'price': 250, 'category': 'Electronics', 'updated_at': ut},\n",
    "    {'product_id': '00002', 'product_name': 'Thermostat', 'price': 400, 'category': 'Electronics', 'updated_at': ut},\n",
    "    {'product_id': '00003', 'product_name': 'Television', 'price': 600, 'category': 'Electronics', 'updated_at': ut},\n",
    "    {'product_id': '00004', 'product_name': 'Blender', 'price': 100, 'category': 'Electronics', 'updated_at': ut},\n",
    "    {'product_id': '00005', 'product_name': 'USB charger', 'price': 50, 'category': 'Electronics', 'updated_at': ut}\n",
    "]\n",
    "\n",
    "df_products = spark.createDataFrame(Row(**x) for x in product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ab9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_options = {\n",
    "    'hoodie.table.name': table_name,\n",
    "    'hoodie.datasource.write.storage.type': 'COPY_ON_WRITE',\n",
    "    'hoodie.datasource.write.recordkey.field': 'product_id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'category',\n",
    "    'hoodie.datasource.write.table.name': table_name,\n",
    "    'hoodie.datasource.write.operation': 'upsert',\n",
    "    'hoodie.datasource.write.precombine.field': 'updated_at',\n",
    "    'hoodie.datasource.write.hive_style_partitioning': 'true',\n",
    "    'hoodie.upsert.shuffle.parallelism': 2,\n",
    "    'hoodie.insert.shuffle.parallelism': 2,\n",
    "    'path': table_location,\n",
    "    'hoodie.datasource.hive_sync.enable': 'true',\n",
    "    'hoodie.datasource.hive_sync.database': database_name,\n",
    "    'hoodie.datasource.hive_sync.table': table_name,\n",
    "    'hoodie.datasource.hive_sync.partition_fields': 'category',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n",
    "    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n",
    "    'hoodie.datasource.hive_sync.mode': 'hms'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5380042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.write.format(\"hudi\")  \\\n",
    "    .options(**hudi_options)  \\\n",
    "    .mode(\"overwrite\")  \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c98800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
