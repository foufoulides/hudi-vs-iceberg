{
	"metadata": {
		"colab": {
			"collapsed_sections": [],
			"name": "hudi_dataframe.ipynb",
			"provenance": []
		},
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 5,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%session_id_prefix native-hudi-dataframe-\n%glue_version 3.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\",\n  \"--datalake-formats\": \"hudi\"\n}",
			"metadata": {
				"id": "ec9c4962-e6d5-4029-9913-52dfd34eefd2",
				"outputId": "cb6298b3-cf2f-486c-b30b-339f3e2bb959",
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.3 \nSetting session ID prefix to native-hudi-dataframe-\nSetting Glue version to: 3.0\nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false', '--datalake-formats': 'hudi'}\n",
					"output_type": "stream"
				}
			],
			"id": "ec9c4962-e6d5-4029-9913-52dfd34eefd2"
		},
		{
			"cell_type": "code",
			"source": "bucket_name = \"mojap-raw-hist-sandbox-608\"\nbucket_prefix = \"hmpps/oasys/EOR/BACKUP_SYNTHETIC/OASYS_QUESTION_SANDBOXY\"\ndatabase_name = \"hudi_df\"\ntable_name = \"oasys_synth_q\"\ntable_prefix = f\"{bucket_prefix}/{database_name}/{table_name}\"\ntable_location = f\"s3://{bucket_name}/{table_prefix}\"",
			"metadata": {
				"id": "74b7200d-2175-4f2f-b7ff-569ce57fd192",
				"outputId": "59a307cc-ed32-406f-afc8-5bebee80bb39",
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::684969100054:role/AdminAccessGlueNotebook\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 3b9682a6-b03a-4af2-ab71-e47d0ccb57e2\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.3\n--enable-glue-datacatalog true\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\n--datalake-formats hudi\nWaiting for session 3b9682a6-b03a-4af2-ab71-e47d0ccb57e2 to get into ready status...\nSession 3b9682a6-b03a-4af2-ab71-e47d0ccb57e2 has been created.\n\n",
					"output_type": "stream"
				}
			],
			"id": "74b7200d-2175-4f2f-b7ff-569ce57fd192"
		},
		{
			"cell_type": "markdown",
			"source": "## Clean up existing resources",
			"metadata": {
				"id": "7e7eab99-9d36-4b5b-8eb0-d7b935351750"
			},
			"id": "7e7eab99-9d36-4b5b-8eb0-d7b935351750"
		},
		{
			"cell_type": "code",
			"source": "import boto3\n\n## Create a database with the name hudi_df to host hudi tables if not exists.\ntry:\n    glue = boto3.client('glue')\n    glue.create_database(DatabaseInput={'Name': database_name})\n    print(f\"New database {database_name} created\")\nexcept glue.exceptions.AlreadyExistsException:\n    print(f\"Database {database_name} already exist\")\n\n## Delete files in S3\ns3 = boto3.resource('s3')\nbucket = s3.Bucket(bucket_name)\nbucket.objects.filter(Prefix=f\"{table_prefix}/\").delete()\n\n## Drop table in Glue Data Catalog\ntry:\n    glue = boto3.client('glue')\n    glue.delete_table(DatabaseName=database_name, Name=table_name)\nexcept glue.exceptions.EntityNotFoundException:\n    print(f\"Table {database_name}.{table_name} does not exist\")\n",
			"metadata": {
				"id": "62dc44d1-4c48-4f24-bfce-60637972914b",
				"outputId": "c26902eb-9339-4bbe-88b2-d81cebd5f699",
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Database hudi_df already exist\n{'ResponseMetadata': {'RequestId': '3e6ca492-8bb3-45bc-9007-78982fb4e178', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 18 May 2023 07:34:43 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': '3e6ca492-8bb3-45bc-9007-78982fb4e178'}, 'RetryAttempts': 0}}\n",
					"output_type": "stream"
				}
			],
			"id": "62dc44d1-4c48-4f24-bfce-60637972914b"
		},
		{
			"cell_type": "code",
			"source": "import json",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "078bf366-869c-4f7f-be41-ea2b3f5576ba"
		},
		{
			"cell_type": "markdown",
			"source": "## Create Hudi table with sample data using catalog sync",
			"metadata": {
				"id": "08706080-9af8-4721-bdfa-0872e0407c68"
			},
			"id": "08706080-9af8-4721-bdfa-0872e0407c68"
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nimport time\nfrom pyspark.sql.functions import col, to_timestamp,lit\nut = time.time()\n\n\ndf_products = spark.read.parquet(\"s3://mojap-raw-hist-sandbox-608/hmpps/oasys/EOR/BACKUP_SYNTHETIC/OASYS_QUESTION_SANDBOXY/\")\n# Convert the date_string column to timestamp\ndf_products = df_products.withColumn(\"date_timestamp\", to_timestamp(col(\"extraction_timestamp\"), 'yyyy-MM-dd'))\ndf_products = df_products.withColumn(\"op\", lit(None).cast(\"string\"))\n\ndf_initial_full_load = df_products.sample(0.0001)\n",
			"metadata": {
				"id": "1d241e37-0ab5-4e1d-9ec1-fd428bc865e8",
				"outputId": "21c3354e-da8d-48de-8a76-6f73b24317dc",
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "1d241e37-0ab5-4e1d-9ec1-fd428bc865e8"
		},
		{
			"cell_type": "markdown",
			"source": "## Create a dataframe with a sample of the initial dataframe that has some additional columns!",
			"metadata": {},
			"id": "bdaf4bb2-d66c-435d-b05e-16c810c5e936"
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import rand\n\ndef add_mixed_columns(df, num_cols, ratio):\n    num_string_cols = int(num_cols * ratio)\n    num_int_cols = num_cols - num_string_cols\n\n    # Add string columns\n    for i in range(1, num_string_cols + 1):\n        new_col_name = f'col_{i}'\n        df = df.withColumn(new_col_name, lit(\"stringdata\"))\n\n    # Add integer columns\n    for i in range(num_string_cols + 1, num_cols + 1):\n        new_col_name = f'intcol_{i - num_string_cols}'\n        # rand() generates a number between 0 and 1. We multiply it by 100 and convert to integer to get random integers.\n        df = df.withColumn(new_col_name, (rand() * 100).cast(\"integer\"))\n\n    return df\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "e6c19976-77e9-4f76-932b-0a061a2eb4d6"
		},
		{
			"cell_type": "code",
			"source": "\ndf_anothersample = df_products.sample(0.0001) # another sample with differentg rows\ndf_anothersample = df_anothersample.withColumn(\"date_timestamp\", to_timestamp(lit('2023-05-18')) )\n\ndf_updated_schema = add_mixed_columns(df_anothersample,10,0.5)\n\ndf_updated_schema.printSchema()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- op: string (nullable = true)\n |-- extraction_timestamp: string (nullable = true)\n |-- scn: string (nullable = true)\n |-- oasys_question_pk: decimal(38,10) (nullable = true)\n |-- date_timestamp: timestamp (nullable = true)\n |-- col_1: string (nullable = false)\n |-- col_2: string (nullable = false)\n |-- col_3: string (nullable = false)\n |-- col_4: string (nullable = false)\n |-- col_5: string (nullable = false)\n |-- intcol_1: integer (nullable = true)\n |-- intcol_2: integer (nullable = true)\n |-- intcol_3: integer (nullable = true)\n |-- intcol_4: integer (nullable = true)\n |-- intcol_5: integer (nullable = true)\n",
					"output_type": "stream"
				}
			],
			"id": "de2239dc-6ea4-436b-b744-56f55c9a62d0"
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.types import StructType, StructField, StringType\n\nold_schema = df_updated_schema.schema\n\nnew_fields = []\nfor field in old_schema.fields:\n    if isinstance(field.dataType, StringType):\n        new_fields.append(StructField(field.name, field.dataType, nullable=True))\n    else:\n        new_fields.append(field)\n\nnew_schema = StructType(fields=new_fields)\n\ndf_updated_schema = spark.createDataFrame(df_updated_schema.rdd, schema=new_schema)\ndf_updated_schema.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- op: string (nullable = true)\n |-- extraction_timestamp: string (nullable = true)\n |-- scn: string (nullable = true)\n |-- oasys_question_pk: decimal(38,10) (nullable = true)\n |-- date_timestamp: timestamp (nullable = true)\n |-- col_1: string (nullable = true)\n |-- col_2: string (nullable = true)\n |-- col_3: string (nullable = true)\n |-- col_4: string (nullable = true)\n |-- col_5: string (nullable = true)\n |-- intcol_1: integer (nullable = true)\n |-- intcol_2: integer (nullable = true)\n |-- intcol_3: integer (nullable = true)\n |-- intcol_4: integer (nullable = true)\n |-- intcol_5: integer (nullable = true)\n",
					"output_type": "stream"
				}
			],
			"id": "0cdb4ef7-65fe-4c88-b664-fb2bfd3bb5b3"
		},
		{
			"cell_type": "code",
			"source": "df_initial_full_load.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- op: string (nullable = true)\n |-- extraction_timestamp: string (nullable = true)\n |-- scn: string (nullable = true)\n |-- oasys_question_pk: decimal(38,10) (nullable = true)\n |-- date_timestamp: timestamp (nullable = true)\n",
					"output_type": "stream"
				}
			],
			"id": "af1ba652-d9d7-4363-8fd2-11e69d6b4aad"
		},
		{
			"cell_type": "code",
			"source": "\ndf_initial_full_load.show(1,vertical=True)\ndf_updated_schema.show(1,vertical=True)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "-RECORD 0-----------------------------------\n op                   | null                \n extraction_timestamp | 2023-04-01          \n scn                  | SYNTHETICS          \n oasys_question_pk    | 8975927.2860468380  \n date_timestamp       | 2023-04-01 00:00:00 \nonly showing top 1 row\n\n-RECORD 0-----------------------------------\n op                   | null                \n extraction_timestamp | 2023-04-01          \n scn                  | SYNTHETICS          \n oasys_question_pk    | 8976182.0156663790  \n date_timestamp       | 2023-05-18 00:00:00 \n col_1                | stringdata          \n col_2                | stringdata          \n col_3                | stringdata          \n col_4                | stringdata          \n col_5                | stringdata          \n intcol_1             | 19                  \n intcol_2             | 55                  \n intcol_3             | 65                  \n intcol_4             | 61                  \n intcol_5             | 62                  \nonly showing top 1 row\n",
					"output_type": "stream"
				}
			],
			"id": "01c61830-730e-4a94-a081-54f644e145aa"
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": [],
			"id": "bfa7eaa9-78b0-4ee1-8aa6-6f2aafd4fc3f"
		},
		{
			"cell_type": "code",
			"source": "df_initial_full_load.count()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "1585\n",
					"output_type": "stream"
				}
			],
			"id": "12c44dec-8aab-4809-9965-3e6e41233a09"
		},
		{
			"cell_type": "code",
			"source": "df_updated_schema.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "1679\n",
					"output_type": "stream"
				}
			],
			"id": "5046ee3f-bfd7-4c7a-bc15-ff90f45afc1a"
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": [],
			"id": "c9748c0c-fa2d-4b72-a2e1-895821e7e95a"
		},
		{
			"cell_type": "code",
			"source": "table_location = f\"s3://mojap-raw-hist-sandbox-608/hmpps/oasys/EOR/BACKUP_SYNTHETIC/HUDIEXAMPLE\"\ntable_location",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "'s3://mojap-raw-hist-sandbox-608/hmpps/oasys/EOR/BACKUP_SYNTHETIC/HUDIEXAMPLE'\n",
					"output_type": "stream"
				}
			],
			"id": "22722d66-288e-4845-a67e-907426e338cc"
		},
		{
			"cell_type": "code",
			"source": "hudi_options = {\n    'hoodie.table.name': table_name,\n    'hoodie.datasource.write.storage.type': 'COPY_ON_WRITE',\n    'hoodie.datasource.write.recordkey.field': 'oasys_question_pk',\n    'hoodie.datasource.write.partitionpath.field': 'scn',\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'date_timestamp',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n    'hoodie.upsert.shuffle.parallelism': 2,\n    'hoodie.insert.shuffle.parallelism': 2,\n    'path': table_location,\n    'hoodie.datasource.hive_sync.enable': 'true',\n    'hoodie.datasource.hive_sync.database': database_name,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.partition_fields': 'scn',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.mode': 'hms'\n}\n",
			"metadata": {
				"id": "a5c89612-6971-413a-8bec-29be15404bad",
				"outputId": "f89a9f6b-87f4-472d-e176-b67d6af4b78d",
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "a5c89612-6971-413a-8bec-29be15404bad"
		},
		{
			"cell_type": "code",
			"source": "df_initial_full_load.write.format(\"hudi\")  \\\n    .options(**hudi_options)  \\\n    .mode(\"overwrite\")  \\\n    .save()",
			"metadata": {
				"id": "9e4eca5d-b71a-43f4-963a-7841fff73c8a",
				"outputId": "938e0afa-5d43-47fe-b994-b080df797aed",
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "9e4eca5d-b71a-43f4-963a-7841fff73c8a"
		},
		{
			"cell_type": "markdown",
			"source": "## note : for initial load the following two options need to be configured \n```.option('hoodie.datasource.write.operation', 'upsert')``` on hudi options\n\nand \n\n``` .mode(\"overwrite\")  ``` on  write statement",
			"metadata": {},
			"id": "c4d1e146-1e19-495f-928f-4994c1881755"
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "88b8e9b5-3d70-45e2-96e9-750d290e8311"
		},
		{
			"cell_type": "markdown",
			"source": "## Read from Hudi table",
			"metadata": {
				"id": "fda19a10-4b69-4272-99a2-1b1156e937c2"
			},
			"id": "fda19a10-4b69-4272-99a2-1b1156e937c2"
		},
		{
			"cell_type": "code",
			"source": "df_products_read = spark.read  \\\n    .format(\"hudi\")  \\\n    .load(table_location)\ndf_products_read.show(1,vertical=True,truncate=False)\n",
			"metadata": {
				"id": "14a6f45f-1cf6-4f6f-9e63-4c89d6ce2cbd",
				"outputId": "01e366c7-2545-434a-c972-f429eac04343",
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "-RECORD 0-------------------------------------------------------------------------------------------\n _hoodie_commit_time    | 20230518073708690                                                         \n _hoodie_commit_seqno   | 20230518073708690_0_1                                                     \n _hoodie_record_key     | 8975939.9683490770                                                        \n _hoodie_partition_path | scn=SYNTHETICS                                                            \n _hoodie_file_name      | 59255b78-cbab-4ebe-a05e-0a8cc817784e-0_0-35-177_20230518073708690.parquet \n op                     | null                                                                      \n extraction_timestamp   | 2023-04-01                                                                \n oasys_question_pk      | 8975939.9683490770                                                        \n date_timestamp         | 2023-04-01 00:00:00                                                       \n scn                    | SYNTHETICS                                                                \nonly showing top 1 row\n",
					"output_type": "stream"
				}
			],
			"id": "14a6f45f-1cf6-4f6f-9e63-4c89d6ce2cbd"
		},
		{
			"cell_type": "code",
			"source": "df_products_read.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "1585\n",
					"output_type": "stream"
				}
			],
			"id": "2ffd8ad3-cfb5-4609-899b-76dd4933a9da"
		},
		{
			"cell_type": "markdown",
			"source": "## Upsert records into Hudi table with evolved Schema",
			"metadata": {
				"id": "c013d49c-da63-4910-b423-4ebd0e346e1f"
			},
			"id": "c013d49c-da63-4910-b423-4ebd0e346e1f"
		},
		{
			"cell_type": "markdown",
			"source": "\n",
			"metadata": {
				"id": "97c52ecd-ac33-4178-b41d-ede0db0b1c97",
				"outputId": "29c34dd9-ec88-49d9-ccaf-5a529abb0050"
			},
			"id": "041a08d9-a726-4bfc-80ce-429f8a9fa056"
		},
		{
			"cell_type": "code",
			"source": "\n#Hudi options from previous operation have the right hudi options\nprint(json.dumps(hudi_options, indent=4))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "{\n    \"hoodie.table.name\": \"oasys_synth_q\",\n    \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n    \"hoodie.datasource.write.recordkey.field\": \"oasys_question_pk\",\n    \"hoodie.datasource.write.partitionpath.field\": \"scn\",\n    \"hoodie.datasource.write.table.name\": \"oasys_synth_q\",\n    \"hoodie.datasource.write.operation\": \"upsert\",\n    \"hoodie.datasource.write.precombine.field\": \"date_timestamp\",\n    \"hoodie.datasource.write.hive_style_partitioning\": \"true\",\n    \"hoodie.upsert.shuffle.parallelism\": 2,\n    \"hoodie.insert.shuffle.parallelism\": 2,\n    \"path\": \"s3://mojap-raw-hist-sandbox-608/hmpps/oasys/EOR/BACKUP_SYNTHETIC/HUDIEXAMPLE\",\n    \"hoodie.datasource.hive_sync.enable\": \"true\",\n    \"hoodie.datasource.hive_sync.database\": \"hudi_df\",\n    \"hoodie.datasource.hive_sync.table\": \"oasys_synth_q\",\n    \"hoodie.datasource.hive_sync.partition_fields\": \"scn\",\n    \"hoodie.datasource.hive_sync.partition_extractor_class\": \"org.apache.hudi.hive.MultiPartKeysValueExtractor\",\n    \"hoodie.datasource.hive_sync.use_jdbc\": \"false\",\n    \"hoodie.datasource.hive_sync.mode\": \"hms\"\n}\n",
					"output_type": "stream"
				}
			],
			"id": "df457882-d0d6-4a71-a5e2-968d5cb4a913"
		},
		{
			"cell_type": "code",
			"source": "df_updated_schema.write.format(\"hudi\") \\\n    .options(**hudi_options) \\\n    .mode(\"append\") \\\n    .save()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "e81e32da-54f1-448b-81e8-fb251d77dd85"
		},
		{
			"cell_type": "code",
			"source": "df_updated_schema_read = spark.read  \\\n    .format(\"hudi\")  \\\n    .load(table_location)\n",
			"metadata": {
				"id": "72b1d420-a08e-4f8f-9c81-69e8ef6fea42",
				"outputId": "9c3f2af0-4261-4205-a683-435825c00b4b",
				"trusted": true
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "72b1d420-a08e-4f8f-9c81-69e8ef6fea42"
		},
		{
			"cell_type": "code",
			"source": "df_updated_schema_read.count()\n\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "3264\n",
					"output_type": "stream"
				}
			],
			"id": "6b9a922b-5f93-4186-b4d7-5acb76856e7f"
		},
		{
			"cell_type": "code",
			"source": "df_updated_schema_read.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- _hoodie_commit_time: string (nullable = true)\n |-- _hoodie_commit_seqno: string (nullable = true)\n |-- _hoodie_record_key: string (nullable = true)\n |-- _hoodie_partition_path: string (nullable = true)\n |-- _hoodie_file_name: string (nullable = true)\n |-- op: string (nullable = true)\n |-- extraction_timestamp: string (nullable = true)\n |-- oasys_question_pk: decimal(38,10) (nullable = true)\n |-- date_timestamp: timestamp (nullable = true)\n |-- col_1: string (nullable = true)\n |-- col_2: string (nullable = true)\n |-- col_3: string (nullable = true)\n |-- col_4: string (nullable = true)\n |-- col_5: string (nullable = true)\n |-- intcol_1: integer (nullable = true)\n |-- intcol_2: integer (nullable = true)\n |-- intcol_3: integer (nullable = true)\n |-- intcol_4: integer (nullable = true)\n |-- intcol_5: integer (nullable = true)\n |-- scn: string (nullable = true)\n",
					"output_type": "stream"
				}
			],
			"id": "0f55d553-4d9d-4d00-9b1c-b6af8bd4a91b"
		},
		{
			"cell_type": "code",
			"source": "df_updated_schema_read.show(1,vertical=True,truncate=False)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "-RECORD 0-------------------------------------------------------------------------------------------\n _hoodie_commit_time    | 20230518073708690                                                         \n _hoodie_commit_seqno   | 20230518073708690_0_1                                                     \n _hoodie_record_key     | 8975939.9683490770                                                        \n _hoodie_partition_path | scn=SYNTHETICS                                                            \n _hoodie_file_name      | 59255b78-cbab-4ebe-a05e-0a8cc817784e-0_0-35-177_20230518073708690.parquet \n op                     | null                                                                      \n extraction_timestamp   | 2023-04-01                                                                \n oasys_question_pk      | 8975939.9683490770                                                        \n date_timestamp         | 2023-04-01 00:00:00                                                       \n col_1                  | null                                                                      \n col_2                  | null                                                                      \n col_3                  | null                                                                      \n col_4                  | null                                                                      \n col_5                  | null                                                                      \n intcol_1               | null                                                                      \n intcol_2               | null                                                                      \n intcol_3               | null                                                                      \n intcol_4               | null                                                                      \n intcol_5               | null                                                                      \n scn                    | SYNTHETICS                                                                \nonly showing top 1 row\n",
					"output_type": "stream"
				}
			],
			"id": "91486521-7428-4ba2-85f5-bb0de31e3e7f"
		},
		{
			"cell_type": "markdown",
			"source": "## Stop Session",
			"metadata": {},
			"id": "68344ffa"
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"id": "7d6bc232-eef6-4493-8c79-812cd73a17f0",
				"outputId": "2a8cd8f5-29ca-4bad-9fc5-9658776b2ee2",
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 3b9682a6-b03a-4af2-ab71-e47d0ccb57e2\nStopped session.\n",
					"output_type": "stream"
				}
			],
			"id": "7d6bc232-eef6-4493-8c79-812cd73a17f0"
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "bf885bf2-c4f2-4b11-9f5f-315319d05dbb"
		}
	]
}